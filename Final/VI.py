# -*- coding: utf-8 -*-
"""final_hw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18i4ZlZbIoVk9yxf-dLewMTm542_jUWg-
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
from math import cos, sin, pi
from sklearn.mixture import GaussianMixture
from sklearn.neighbors import KernelDensity

def sampling_from_gaussian(num_points, K, R):
  means = [[R*cos(2*pi*k/K), R*sin(2*pi*k/K)] for k in range(1,11)]
  points = []

  points_per_cluster = [0] * K
  for i in range(num_points):
    points_per_cluster[np.random.randint(0, K)] += 1

  for i in range(K):
    points.append(sampling_from_cluster(points_per_cluster[i], means[i]))
  return torch.from_numpy(np.concatenate(points)), torch.from_numpy(np.array(means))

def sampling_from_cluster(num_points, means):
  return np.random.normal(loc=means, scale=1, size=(num_points, len(means)))

def plot_means(truth, pred):
  plt.scatter(truth[:, 0], truth[:, 1], c='r', label='truth')
  plt.scatter(pred[:, 0], pred[:, 1], c='b', label='pred')
  plt.legend(['Truth', 'Predicted'])
  plt.grid()
  plt.title('Comparison of truth and predicted means computed by EM')
  plt.show()

def plot_pis(pred, K):
  truth = np.array([1/K] * K )
  x = np.arange(1, K+1)
  plt.scatter(x, truth, c='r')
  plt.scatter(x, pred, c='b')
  plt.ylim([0, 2/K])
  plt.legend(['Truth', 'Predicted'])
  plt.grid()
  plt.title('Comparison of truth and predicted weights computed by EM')
  plt.show()

def fit_kd(x, bandwidth=1.0):
  KD = KernelDensity(bandwidth=bandwidth, kernel='gaussian').fit(x)
  return KD

def plot_performance(bandwidths, band_idx, log_likelihoods, test_score):
  plt.scatter(bandwidths, log_likelihoods, c='b', s=3)
  plt.scatter(bandwidths[band_idx], log_likelihoods[band_idx], c='r', s=8)
  plt.scatter(bandwidths[band_idx], test_score, c='g', s=8)
  plt.xticks(bandwidths[::2])
  plt.legend(['Validation Scores', 'Max Validation Score', 'Test Validation Score'])
  plt.grid()
  plt.title(f'Log Likelihood Performance, optimal bandwidth={bandwidths[band_idx]}')
  plt.show()

def validate_kd(x, num_bandwidth=20):
  
  kd_estimators = []
  log_likelihoods = np.zeros(num_bandwidth)
  X_val, means_val = sampling_from_gaussian(1000, 10, 10)

  bandwidths = np.linspace(0.1, 5, num_bandwidth)

  for i in range(num_bandwidth):
    # Training the model
    kd = fit_kd(X_train, bandwidths[i])
    kd_estimators.append(kd)
    # Validating the model
    perf = kd.score(X_val) / len(X_val)
    log_likelihoods[i]= perf
  
  # Selecting the model with the best performance
  band_idx = np.argmax(log_likelihoods)
  print(f'In validation, model with bandwidth {bandwidths[band_idx]} has the highest score of {log_likelihoods[band_idx]}')
  kd = kd_estimators[band_idx]

  # Testing the best model
  X_test, means_test = sampling_from_gaussian(1000, 10, 10)
  test_score = kd.score(X_test) / len(X_test)
  print(f'In test, model has a score of {test_score}') 

  plot_performance(bandwidths, band_idx, log_likelihoods, test_score)

  return kd



# Generating a sample
X_train, means = sampling_from_gaussian(1000, 10, 10)
plt.scatter(X_train[:, 0], X_train[:, 1])
plt.show()

# Fit a GMM to the generated dataset as baseline
gmm = GaussianMixture(n_components=10, covariance_type='spherical', tol=1e-4).fit(X_train)

plot_means(means, gmm.means_)

plot_pis(gmm.weights_, 10)

pnts, means = sampling_from_gaussian(1000, 10, 1)
plt.scatter(pnts[:, 0], pnts[:, 1])
plt.show()

gmm = GaussianMixture(n_components=10, covariance_type='tied', tol=1e-4).fit(pnts)

plot_means(means, gmm.means_)

plot_pis(gmm.weights_, 10)

# Training & Validating a Kernel Density model
X_train, means = sampling_from_gaussian(1000, 10, 10)
kd = validate_kd(X_train, num_bandwidth=20)

# Energy Based Model

import torch
import torch.nn as nn

class EBM(nn.Module):
  def __init__(self, input_size, output_size=1, hidden_size=128):
    super(EBM, self).__init__()
    self.fc = nn.Sequential(
        nn.Linear(input_size, hidden_size),
        nn.ReLU(),
        nn.Linear(hidden_size, hidden_size),
        nn.ReLU(),
        nn.Linear(hidden_size, output_size)
    )
  
  def forward(self, x):
    x = self.fc(x)
    return x


X_M, means_xm = sampling_from_gaussian(5000, 10, 10)
print(X_M.shape)

# Training the EBM

epochs = 5
lr = 0.01
model = EBM(input_size=2, output_size=1)
for epoch in range(epochs):
  M = X_M.shape[0]
  # Here M refers to the newly generated data for importance sampling

  grad_loss = []

  for p in model.parameters():
    N = 0
    D = 0
    for x in X_M:
      model.zero_grad()
      energy = -model(x.float())
      energy.backward()
      f_of_xm = p.grad
      prob = torch.Tensor(gmm.predict_proba(x.reshape(1, -1))).squeeze()
      q_x = 1 / prob.shape[0] * torch.sum(prob)
      N += f_of_xm * torch.exp(energy) / q_x
      D += torch.exp(energy) / q_x

    N /= M
    D /= M
    F = N / D

    # Here the n refers the training set fitted on the GMM
    n = X_train.shape[0]
    grad_E = 0
    for x in X_train:
      model.zero_grad()
      energy = -model(x.float())
      energy.backward()
      grad_E += p.grad

    grad_E /= n
    grad_loss.append(grad_E + F)
  print(f'Epoch {epoch} completed training')
  state_dict = model.state_dict()
  for i, (_, param) in enumerate(state_dict.items()):
    param.copy_(param - lr*grad_loss[i])

#from torchsummary import summary
#print(summary(model, X_train.shape))

# Plotting the distribution of X_M
plt.scatter(X_M[:, 0], X_M[:, 1])
plt.show()

import torch.nn.functional as F

X_test, means_test = sampling_from_gaussian(1000, 10, 10)
n = 1000
model.eval()
energies = model(X_test.float()).view(-1)

pred_ebm = F.softmax(-energies, dim=0)
ebm_score = torch.sum(torch.log(pred_ebm)) / n
kd_score = kd.score(X_test) / n

print(f'Energy Based Model Score: {ebm_score}')
print(f'Kernel Density Score: {kd_score}')


"""Variational Inference"""

import scipy.stats as stats

loc1, scale1, size1 = (-5, 5, 175)
loc2, scale2, size2 = (10, 3, 50)

x2 = np.concatenate([np.random.normal(loc=loc1, scale=scale1, size=size1),
            np.random.normal(loc=loc2, scale=scale2, size=size2)])

x_eval = np.linspace(x2.min() - 1, x2.max() + 1, 500)

norm_pdf = stats.norm.pdf(x_eval, loc=-10, scale=.8)

bimodal_pdf = stats.norm.pdf(x_eval, loc=loc1, scale=scale1) * float(size1) / x2.size + \
        stats.norm.pdf(x_eval, loc=loc2, scale=scale2) * float(size2) / x2.size

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111)
ax.plot(x_eval, bimodal_pdf, label='Bimodal PDF')
ax.plot(x_eval, norm_pdf, 'r', label='Normal PDF')
ax.legend()
ax.set_title('PDF of a Bimodal distro. and a normal distro')
plt.show()

"""If we define p to be the bimodal distribution, q to be the normal distribution, then $D_{KL}(p||q)$ is computed with `stats.entropy(bimodal_pdf, norm_pdf)`, and $D_{KL}(q||p)$ is computed with `stats.entropy(norm_pdf, bimodal_pdf)`"""

print(stats.entropy(bimodal_pdf, norm_pdf))

print(stats.entropy(norm_pdf, bimodal_pdf))

"""As we can see, $D_{KL}(p||q)>>D_{KL}(q||p)$ in the above results.
"""
